{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text, Web, & Media Analytics Assignment 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from ir_models import BM25, JM_LM, My_PRM\n",
    "from ir_tools import write_scores_to_file\n",
    "from parsing_functions import parse_stop_words, parse_collection, parse_query, parse_query_set, parse_evaluations, parse_ranking_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse in stop words\n",
    "stop_words = parse_stop_words('common-english-words.txt')\n",
    "\n",
    "# Load the document set (series of collection objects)\n",
    "document_set = {}\n",
    "input_path = 'Data_Collection'\n",
    "for collection_path in os.listdir(input_path):\n",
    "    data_key = collection_path.split('_C', 1)[1]\n",
    "    document_set[data_key] = parse_collection(stop_words, os.path.join(input_path, collection_path))\n",
    "\n",
    "# Parse in query set, apply term specificity to parsed queries\n",
    "query_frame = parse_query_set('the50Queries.txt')\n",
    "query_frame['parsed_title'] = query_frame['title'].apply(lambda row: parse_query(row, stop_words))\n",
    "\n",
    "# Experiment to see if adding quarter-weighted frequency of description element helps\n",
    "query_frame['parsed_description'] = query_frame['description'].apply(lambda row: parse_query(row, stop_words) if row is not pd.NA else pd.NA)\n",
    "query_frame['parsed_description'] = query_frame['parsed_description'].apply(lambda row: {k:v/4 for k,v in row.items()} if row is not pd.NA else pd.NA)\n",
    "\n",
    "query_frame['parsed_query'] = query_frame.apply(\n",
    "    lambda row: {**row['parsed_title'], **{k: v for k, v in row['parsed_description'].items() if k not in row['parsed_title']}} if row['parsed_description'] is not pd.NA else row['parsed_title'], \n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: BM25 ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Jelinek-Mercer Language Model ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Pseudo-Relevance Model ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model Testing ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Use Python to implement three models: `BM25`, `JM_LM`, and `My_PRM`, and **test them on the given 50 data collections for the corresponding 50 queries (topics)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Python programs to implement these three models. You can use a .py file (or a .ipynb file) for each model.\n",
    "\n",
    "\n",
    "For each long query, your python programs will produce ranked results and save them into .dat files. For example, for query R107, you can save the ranked results of three models into “BM25_R107Ranking.dat”, “JM_LM_R107Ranking.dat”, and “My_PRM_R107Ranking.dat”, respectively by using the following format:\n",
    "- The first column is the document id (the itemid in the corresponding XML document)\n",
    "- The second column is the document score (or probability).\n",
    "\n",
    "**Describe:** \n",
    "- Python packages or modules (or any open-source software) you used\n",
    "- The data structures used to represent a single document and a set of documents for each model (you can use different data structures for different models).\n",
    "\n",
    "\n",
    "You also need to **test the three models on the given 50 data collections for the 50 queries (topics) by *printing out the top 15 documents* for each data collection (in descending order)**. The **output will also be put in the appendix of your final report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise result dicts\n",
    "BM25_results = {}\n",
    "JM_LM_results = {}\n",
    "My_PRM_results = {}\n",
    "\n",
    "# Loop over queries/collection objects\n",
    "for query_key, collection in document_set.items():\n",
    "    query = query_frame.loc[query_frame['number'] == query_key, 'parsed_query'].iloc[0]  # retrieve weighted query\n",
    "\n",
    "    # Rank documents\n",
    "    BM25_results[query_key] = BM25(collection=collection, query=query)\n",
    "    JM_LM_results[query_key] = JM_LM(collection=collection, query=query)\n",
    "    My_PRM_results[query_key] = My_PRM(weighting_function=BM25, collection=collection, query=query, threshold=0.7, theta=0)  # NOTE: GSCV threshold/theta?\n",
    "\n",
    "    # Save results\n",
    "    write_scores_to_file(BM25_results[query_key], f\"BM25_R{query_key}Ranking\")\n",
    "    write_scores_to_file(JM_LM_results[query_key], f\"JM_LM_R{query_key}Ranking\")\n",
    "    write_scores_to_file(My_PRM_results[query_key], f\"My_PRM_R{query_key}Ranking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_15(model_results):\n",
    "    \"\"\"\n",
    "    Takes the model results, prints out the top-15 sorted by weights.\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate terating over each set of {query:predictions}, where predictions is a dictionary of {doc_id : document weight}\n",
    "    for(query, predictions) in model_results.items():\n",
    "        print('Query' + str(query) + ' (DocID Weight):') # print result header information\n",
    "\n",
    "        # For the given result set, sort the document weights and take the top 15 scores (\"up to n\" indexing doesn't break for lists shorter than n)\n",
    "        sorted_weights_top15 = {doc_id:doc_score for doc_id,doc_score in sorted(predictions.items(), key=lambda item: item[1], reverse=True)[:15]}\n",
    "\n",
    "        # Iterate over each doc_id:weight for the predictions\n",
    "        for (doc_id, weight) in sorted_weights_top15.items():\n",
    "            print(doc_id + ': ' + str(weight))  # print results data\n",
    "\n",
    "        print()  # print linebreak for readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_top_15(BM25_results)\n",
    "get_top_15(JM_LM_results)\n",
    "get_top_15(My_PRM_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Model Evaluation ✔️"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Use three effectiveness measures to evaluate the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you need to **use the relevance judgments (EvaluationBenchmark.zip)** to **compare with the ranking outputs in the folder of “RankingOutputs” for the selected effectiveness metric** for the three models.\n",
    "\n",
    "\n",
    "You need to use the following three different effectiveness measures to evaluate the document ranking results you saved in the folder “RankingOutputs”:\n",
    "1) Average precision (and MAP)\n",
    "2) Precision@10 (and their average)\n",
    "3) Discounted cumulative gain at rank position 10 ($p = 10$), $DCG_{10}$ (and their average):  \n",
    "    $DCG_p=rel_i+\\sum_{i=2}^p\\frac{rel_i}{log_2(i)}$  \n",
    "        $rel_i=1$ if the document at position $i$ is releveant; otherwise, it is 0.\n",
    "\n",
    "Evaluation results can be summarized in tables or graphs. Examples are provided in the sepcification sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parse DATs\n",
    "# BM25_results = parse_ranking_files('RankingOutputs', 'BM25')\n",
    "# JM_LM_results = parse_ranking_files('RankingOutputs', 'JM_LM')\n",
    "# My_PRM_results = parse_ranking_files('RankingOutputs', 'My_PRM')\n",
    "\n",
    "# Parse in evaluation benchmarks\n",
    "evaluations = parse_evaluations('EvaluationBenchmark/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision (MAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(evaluations, model_results, threshold: float, top_k: int = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the precision for each topic in a collection, optionally only considering the top_k results.\n",
    "    If top_k is specified, precision is calculated based on the top_k highest scored documents.\n",
    "    Average Precision across all topics is appended as a final row.\n",
    "    \"\"\"\n",
    "\n",
    "    precisions = []\n",
    "\n",
    "    for topic, relevancy in evaluations.items():\n",
    "        predicted_scores = model_results.get(topic, {})\n",
    "\n",
    "        # Sort and possibly limit the results to top_k if specified\n",
    "        if top_k:\n",
    "            top_items = sorted(predicted_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            filtered_scores = dict(top_items)\n",
    "        else:\n",
    "            filtered_scores = {doc_id: score for doc_id, score in predicted_scores.items() if score > threshold}\n",
    "\n",
    "        # Calculate the number of predicted relevant documents that meet the threshold\n",
    "        retrieved_docs = len(filtered_scores)\n",
    "\n",
    "        # Calculate the number of correctly retrieved documents (true positives)\n",
    "        true_positives = sum(1 for doc_id in filtered_scores.keys() if relevancy.get(doc_id) == 1)\n",
    "\n",
    "        # Calculate precision\n",
    "        precision = true_positives / retrieved_docs if retrieved_docs > 0 else 0\n",
    "\n",
    "        # Append results to list for DataFrame conversion\n",
    "        precisions.append({'topic': topic, 'precision': precision})\n",
    "\n",
    "    # Create DataFrame\n",
    "    precision_df = pd.DataFrame(precisions)\n",
    "\n",
    "    # Calculate MAP (Average) and append as a new row\n",
    "    map_score = precision_df['precision'].mean()\n",
    "    average_row = pd.DataFrame([{'topic': 'MAP' if not top_k else 'Average', 'precision': map_score}])\n",
    "    precision_df = pd.concat([precision_df, average_row], ignore_index=True)\n",
    "\n",
    "    return precision_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining thresholds (used in both average precision and precision@10)\n",
    "bm25_threshold = 0.6\n",
    "jm_lm_threshold = 0.000001\n",
    "prm_threshold = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate precision for each query\n",
    "bm25_precision = calculate_precision(evaluations, BM25_results, bm25_threshold, top_k = None).rename({'precision': 'bm25_precision'}, axis = 1)\n",
    "\n",
    "# Calculate precision for each query\n",
    "jm_lm_precision = calculate_precision(evaluations, JM_LM_results, jm_lm_threshold, top_k = None).rename({'precision': 'jm_lm_precision'}, axis = 1)\n",
    "\n",
    "# Calculate precision for each query\n",
    "prm_precision = calculate_precision(evaluations, My_PRM_results, prm_threshold, top_k = None).rename({'precision': 'prm_precision'}, axis = 1)\n",
    "\n",
    "# Merging results\n",
    "average_precision = pd.merge((pd.merge(bm25_precision, jm_lm_precision, on='topic')), prm_precision, on='topic')\n",
    "average_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision @ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank variable\n",
    "top_k = 10\n",
    "\n",
    "# Calculate precision for each model\n",
    "bm25_precision_10 = calculate_precision(evaluations, BM25_results, bm25_threshold, top_k = top_k).rename({'precision': f'bm25_precision@{top_k}'}, axis = 1)\n",
    "jm_lm_precision_10 = calculate_precision(evaluations, JM_LM_results, jm_lm_threshold, top_k = top_k).rename({'precision': f'jm_lm_precision@{top_k}'}, axis = 1)\n",
    "prm_precision_10 = calculate_precision(evaluations, My_PRM_results, prm_threshold, top_k = top_k).rename({'precision': f'prm_precision@{top_k}'}, axis = 1)\n",
    "\n",
    "# Merging results\n",
    "precision_10 = pd.merge((pd.merge(bm25_precision_10, jm_lm_precision_10, on='topic')), prm_precision_10, on='topic')\n",
    "precision_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCG @ 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dcg(evaluations, model_results, threshold, p):\n",
    "    \"\"\"\n",
    "    Calculate the Discounted Cumulative Gain (DCG) at rank p (effectively top k) for each topic in a collection.\n",
    "    DCG is calculated using a logarithmic discount factor to decrease the weight of relevance for documents retrieved later in the list.\n",
    "    \"\"\"\n",
    "\n",
    "    dcgs = []\n",
    "\n",
    "    for topic, relevancy in evaluations.items():\n",
    "        predicted_scores = model_results.get(topic, {})\n",
    "\n",
    "        # Sort predicted scores and limit results to top p\n",
    "        top_p_scores = sorted(predicted_scores.items(), key=lambda x: x[1], reverse=True)[:p]\n",
    "\n",
    "        # Calculate DCG using the logarithmic discount\n",
    "        dcg = 0\n",
    "        for rank, (doc_id, score) in enumerate(top_p_scores, start=1):\n",
    "            relevance = 1 if score > threshold and relevancy.get(doc_id, 0) == 1 else 0\n",
    "            if rank == 1:\n",
    "                dcg += relevance  # no discount for the first item\n",
    "            else:\n",
    "                dcg += relevance / np.log2(rank)  # discounting starts from the second item\n",
    "\n",
    "        # Append results to list for DataFrame conversion\n",
    "        dcgs.append({'topic': topic, 'DCG': dcg})\n",
    "\n",
    "    # Create DataFrame\n",
    "    dcg_df = pd.DataFrame(dcgs)\n",
    "\n",
    "    # Calculate Average DCG and append as a new row\n",
    "    average_dcg = dcg_df['DCG'].mean()\n",
    "    average_row = pd.DataFrame([{'topic': 'Average DCG', 'DCG': average_dcg}])\n",
    "    dcg_df = pd.concat([dcg_df, average_row], ignore_index=True)\n",
    "\n",
    "    return dcg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rank variable\n",
    "p = 10\n",
    "\n",
    "# Calculate precision for each model\n",
    "bm25_dcg_10 = calculate_dcg(evaluations, BM25_results, bm25_threshold, p = p).rename({'DCG': f'bm25_DCG_p{p}'}, axis = 1)\n",
    "jm_lm_dcg_10 = calculate_dcg(evaluations, JM_LM_results, jm_lm_threshold, p = p).rename({'DCG': f'jm_lm_DCG_p{p}'}, axis = 1)\n",
    "prm_dcg_10 = calculate_dcg(evaluations, My_PRM_results, prm_threshold, p = p).rename({'DCG': f'prm_DCG_p{p}'}, axis = 1)\n",
    "\n",
    "# Merging results\n",
    "dcg_10 = pd.merge((pd.merge(bm25_dcg_10, jm_lm_dcg_10, on='topic')), prm_dcg_10, on='topic')\n",
    "dcg_10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 Optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_f1_score(evaluations, model_results, threshold: float, beta: float = 1.0, top_k: int = None) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the weighted F1 score across all topics for a given threshold and beta, optionally considering only the top_k results.\n",
    "    \"\"\"\n",
    "    total_f1_score = 0\n",
    "    count = 0\n",
    "\n",
    "    for topic, relevancy in evaluations.items():\n",
    "        predicted_scores = model_results.get(topic, {})\n",
    "\n",
    "        # Sort and possibly limit the results to top_k if specified\n",
    "        if top_k:\n",
    "            top_items = sorted(predicted_scores.items(), key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            filtered_scores = dict(top_items)  # Convert sorted list back to dict\n",
    "        else:\n",
    "            filtered_scores = {doc_id: score for doc_id, score in predicted_scores.items() if score > threshold}\n",
    "\n",
    "        # Calculate true positives, false positives, and false negatives\n",
    "        true_positives = sum(1 for doc_id in filtered_scores.keys() if relevancy.get(doc_id) == 1)\n",
    "        false_positives = sum(1 for doc_id in filtered_scores.keys() if relevancy.get(doc_id) == 0)\n",
    "        false_negatives = sum(1 for doc_id, is_relevant in relevancy.items() if is_relevant == 1 and doc_id not in filtered_scores)\n",
    "\n",
    "        # Calculate precision and recall\n",
    "        precision = true_positives / (true_positives + false_positives) if true_positives + false_positives > 0 else 0\n",
    "        recall = true_positives / (true_positives + false_negatives) if true_positives + false_negatives > 0 else 0\n",
    "\n",
    "        # Calculate weighted F1 score\n",
    "        if (beta**2 * precision + recall) != 0:\n",
    "            f1_score = (1 + beta**2) * (precision * recall) / (beta**2 * precision + recall)\n",
    "        else:\n",
    "            f1_score = 0\n",
    "\n",
    "        # Add to total F1 score and increment count\n",
    "        total_f1_score += f1_score\n",
    "        count += 1\n",
    "\n",
    "    # Calculate average F1 score across all topics\n",
    "    average_f1_score = total_f1_score / count if count > 0 else 0\n",
    "    return average_f1_score\n",
    "\n",
    "def manual_grid_search(evaluations, model_results, thresholds, top_ks: list = None):\n",
    "    # Define thresholds and top_k values to be tested\n",
    "\n",
    "    best_score = -float('inf')  # Assuming higher score is better; adjust if needed\n",
    "    best_params = {}\n",
    "\n",
    "    # Loop over all combinations of threshold and top_k\n",
    "    for threshold in thresholds:\n",
    "        if top_ks:\n",
    "            for top_k in top_ks:\n",
    "                # Calculate average precision for the current combination of threshold and top_k\n",
    "                average_f1  = calculate_weighted_f1_score(evaluations, model_results, threshold, top_k)\n",
    "                \n",
    "                # Check if the current score is better than what we've seen and update best score and parameters\n",
    "                if average_f1  > best_score:\n",
    "                    best_score = average_f1 \n",
    "                    best_params = {'threshold': threshold}\n",
    "        \n",
    "        else:\n",
    "            # Calculate average precision for the current combination of threshold\n",
    "            average_f1  = calculate_weighted_f1_score(evaluations, model_results, threshold)\n",
    "            \n",
    "            # Check if the current score is better than what we've seen and update best score and parameters\n",
    "            if average_f1  > best_score:\n",
    "                best_score = average_f1\n",
    "                best_params = {'threshold': threshold}\n",
    "\n",
    "    return best_score, best_params\n",
    "\n",
    "# Build threshold list\n",
    "low_threshold = 0.0000001\n",
    "step_size = 0.0001\n",
    "num_steps = 100\n",
    "thresholds = [low_threshold + i * step_size for i in range(num_steps)]\n",
    "top_ks = [10]\n",
    "manual_grid_search(evaluations, JM_LM_results, thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Recommend a model based on significance test and your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to conduct a significance test to compare models. You can choose a t-test to perform a significance test on the evaluation results (e.g., in Tables 1, 2 and 3). \n",
    "\n",
    "You can compare models between:\n",
    "- **BM25** and **JM_LM**\n",
    "- **BM25** and **My_PRM**\n",
    "- **JM_LM** and **My_PRM**\n",
    "\n",
    "Based on $t$-test results ($p$-value and $t$-statistic), you can recommend a model (You ***want the proposed \"My_RPM\" to be the best because it is your own model***). You can perform the $t$-test using a single effectiveness measure or multiple measures. Generally, using more effectiveness measures provides stronger evidence against the null hypothesis. Note that if the $t$-test is unsatisfactory, you can use the evaluation results to refine **My_PRM** mode. For example, you can adjust parameter settings or update your design and implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
