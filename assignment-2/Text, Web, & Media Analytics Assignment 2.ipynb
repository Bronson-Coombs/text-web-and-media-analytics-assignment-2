{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text, Web, & Media Analytics Assignment 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import regex as re\n",
    "import string\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for Bag-of-Word representation\n",
    "class Rcv1Doc:\n",
    "    def __init__(self, item_id: str):\n",
    "        # Type check to ensure object is initialised correctly\n",
    "        if not isinstance(item_id, str):\n",
    "            raise TypeError(\"item_id: value must be a string.\")\n",
    "            # Technically could work with str or int indexing (for key in collection),\n",
    "            # using *only* str ensures no double-up of pointers\n",
    "            # (e.g. item_id '1' vs item_id 1)\n",
    "\n",
    "        self.doc_id = item_id  # assigning doc_id from 'item_id'\n",
    "        self.terms = {}  # dictionary for terms and their frequencies\n",
    "        self._doc_len = 0  # document length, private attribute\n",
    "\n",
    "    def add_term(self, term: str):\n",
    "        \"\"\"Add a term to the document or update its frequency if it already exists.\"\"\"\n",
    "        \n",
    "        # Type check to ensure term is a str\n",
    "        if not isinstance(term, str):\n",
    "            raise TypeError(\"term: value must be a string.\")\n",
    "        \n",
    "        self.doc_len += 1  # extend doc_len\n",
    "\n",
    "        if term in self.terms:\n",
    "            self.terms[term] += 1  # add frequency if the term exists\n",
    "        else:\n",
    "            self.terms[term] = 1  # if it doesn't exist, add it (setting frequency to 1)\n",
    "        \n",
    "    def get_doc_id(self) -> str:\n",
    "        \"\"\"Return the document ID.\"\"\"\n",
    "        return self.doc_id\n",
    "    \n",
    "    def get_term_list(self, sorted_by_freq: bool = None) -> dict:\n",
    "        \"\"\"\n",
    "        Return a list of terms occurring in the document, optionally sorted by their frequency.\n",
    "        If sorted_by_freq is True, the terms are returned sorted by their frequency in descending order.\n",
    "        If sorted_by_freq is False or None (default), the terms are returned in arbitrary order.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check to ensure sorted_by_freq is either None or a boolean\n",
    "        if not isinstance(sorted_by_freq, (bool, type(None))):\n",
    "            raise TypeError(\"sorted_by_freq: must be a boolean or None.\")\n",
    "\n",
    "        if sorted_by_freq:\n",
    "            # If sorted_by_freq is True\n",
    "            sorted_terms = sorted(self.terms.items(), key=lambda word: word[1], reverse=sorted_by_freq)  # generate a sorted list of terms by frequency\n",
    "            return {term: freq for term, freq in sorted_terms}  # return key:value pairs based on sorted terms\n",
    "        else:\n",
    "            # If sorted_by_freq is False or None, return the terms as is (i.e., unsorted and as they are added in)\n",
    "            return self.terms\n",
    "        \n",
    "    def get_bag_of_words(self, sorted_by_freq: bool = None) -> str:\n",
    "        \"\"\"Return full bag-of-words representation for Rcv1Doc object, including; doc_id, term_count, doc_len, and terms.\"\"\"\n",
    "        \n",
    "        # Type check to ensure sorted_by_freq is either None or a boolean\n",
    "        if not isinstance(sorted_by_freq, (bool, type(None))):\n",
    "            raise TypeError(\"sorted_by_freq: must be a boolean or None.\")\n",
    "\n",
    "        # Defining formatted string for bag-of-word representation\n",
    "        bag_of_words = f\"\"\"doc_id='{self.doc_id}',term_count={len(self.get_term_list())},doc_len={self.doc_len},terms={self.get_term_list(sorted_by_freq)}\"\"\"\n",
    "\n",
    "        return bag_of_words  # return BOW representation; this kind of data can be stored and \"unpacked\" easily\n",
    "    \n",
    "    @property  # accessor (get) method for doc_len\n",
    "    def doc_len(self) -> int:\n",
    "        \"\"\"The doc_len property getter method.\"\"\"\n",
    "        return self._doc_len\n",
    "\n",
    "    @doc_len.setter  # mutator (setter) method for doc_len\n",
    "    def doc_len(self, value: int):\n",
    "        \"\"\"The doc_len property setter method.\"\"\"\n",
    "        if not isinstance(value, int):\n",
    "            raise TypeError(\"doc_len: must be an int.\")\n",
    "        if value < 0:\n",
    "            raise ValueError(\"doc_len: must not be negative.\")\n",
    "        \n",
    "        self._doc_len = value\n",
    "\n",
    "# Define the class for collection of Rcv1Doc objects\n",
    "class Rcv1Coll:\n",
    "    def __init__(self):\n",
    "        self.docs = {}  # initialise dictionary to hold collection (dict) of doc_id:Rcv1Doc\n",
    "\n",
    "        self.term_doc_count = {}  # initialise dictionary to track the number of documents each term appears in\n",
    "\n",
    "    # Method to add a doc (Rcv1Doc object)\n",
    "    def add_doc(self, doc: Rcv1Doc):\n",
    "        \"\"\"Add Rcv1Doc object to the collection, using doc_id as the key, and update the inverted index.\"\"\"\n",
    "\n",
    "        # Type check to ensure doc is a Rcv1Doc object\n",
    "        if not isinstance(doc, Rcv1Doc):\n",
    "            raise TypeError(\"doc: must be an instance of Rcv1Doc.\")\n",
    "        \n",
    "        # Add to the docs dict; key as doc_id and value as Rcv1Doc object (doc_id:Rcv1Doc)\n",
    "        self.docs[doc.get_doc_id()] = doc\n",
    "\n",
    "        # Update term document count for each term\n",
    "        for term in doc.terms:\n",
    "            if term in self.term_doc_count:\n",
    "                self.term_doc_count[term] += 1  # add one if the term exists in the corpus dictionary\n",
    "            else:\n",
    "                self.term_doc_count[term] = 1  # if it does not exist in the corpus dictionary, initialise by setting to 1\n",
    "\n",
    "    # Call Bag-of-Word representation of Rcv1Doc from collection via doc_id\n",
    "    def get_doc_id_bag(self, doc_id: str, sorted_by_freq: bool = None) -> str:\n",
    "        \"\"\"Return the string representation of a Rcv1Doc object by its doc_id.\"\"\"\n",
    "\n",
    "        # Type check to ensure doc_id is a string\n",
    "        if not isinstance(doc_id, str):\n",
    "            raise TypeError(\"doc_id: value must be a str.\")  # Corrected to match the check\n",
    "        \n",
    "        # Type check to ensure sorted_by_freq is either None or a boolean\n",
    "        if not isinstance(sorted_by_freq, (bool, type(None))):\n",
    "            raise TypeError(\"sorted_by_freq: must be a boolean or None.\")\n",
    "        \n",
    "        # Attempt to retrieve the document using the doc_id\n",
    "        doc = self.docs.get(doc_id)\n",
    "        \n",
    "        # Check if the document was found; if not, raise an error\n",
    "        if doc is None:\n",
    "            raise ValueError(f\"doc_id: no document found for '{doc_id}'\")\n",
    "        \n",
    "        # If the document is found, return its bag-of-words representation\n",
    "        return doc.get_bag_of_words(sorted_by_freq)\n",
    "\n",
    "    # List representation of Rcv1Doc.doc_id when called via print(); can be used to quickly get a doc_id to be called via get_doc_id_bag()\n",
    "    def __str__(self):\n",
    "        doc_ids_str = ', '.join(self.docs.keys())  # create a string that lists doc_ids\n",
    "        return f'Rcv1Coll(doc_ids: {doc_ids_str})'  # return the string od doc_ids\n",
    "\n",
    "def stop_word_parser(stop_word_path: str) -> list:\n",
    "    \"\"\"Parse defined list of stop words (assumes txt file with words delimited with ',').\"\"\"\n",
    "\n",
    "    # Type check to ensure stop_word_path is a str\n",
    "    if not isinstance(stop_word_path, str):\n",
    "        raise TypeError(\"stop_word_path: value must be a str.\")\n",
    "    \n",
    "    # NOTE: need attribute check the path exists\n",
    "    \n",
    "    # We know what the format is ahead of time, so not a lot of processing needed;\n",
    "    # i.e., assumes we don't need to make something more robust and that we're using the same txt.\n",
    "\n",
    "    # Open file in read mode\n",
    "    with open(stop_word_path, 'r') as file:\n",
    "        stop_words = file.read()  # read text in given file into stop_words\n",
    "    stop_words = stop_words.lower().split(\",\")  # tokenize stop_words; delimited with ','\n",
    "    stop_words = list(set(stop_words))  # reduce stop_words to uniques\n",
    "    \n",
    "    return stop_words  # return stop_words as a list object\n",
    "\n",
    "def tokenization(words: str) -> list:\n",
    "    \"\"\"Tokenize input text by removing line breaks, numbers, punctuation, normalizing whitespace, stripping leading/trailing spaces, and splitting into lowercased words.\"\"\"\n",
    "\n",
    "    # Type check to ensure words is a str\n",
    "    if not isinstance(words, str):\n",
    "        raise TypeError(\"words: value must be a str.\")\n",
    "\n",
    "    words = words.replace(\"\\n\", \"\")  # don't want line breaks to contribute\n",
    "    words = re.sub(r'\\d', '', words)  # not interested in numbers for this particular task, remove\n",
    "    words = re.sub(f'[{re.escape(string.punctuation)}]', ' ', words)  # not interested in punctuation, remove\n",
    "    words = re.sub(r'\\s+|\\t+|\\v+|\\n+|\\r+|\\f+', ' ', words).strip()  # standardise the whitespaces, remove leading/trailing whitespace\n",
    "    words = words.lower()  # standardise words as lower\n",
    "    words = words.split()  # tokenize, deftault split on space\n",
    "\n",
    "    # Filter out small words; can be important in some queries, usually in combinations, opting not to handle for simplicity.\n",
    "    # For example, with no discrete management of apostrophes (indicating contractions or posession) aside from replacement \n",
    "    # of punctuation with a single space, we will get the following: \"Amelia's\" → [\"Amelia\", \"s\"] → [\"Amelia\"].\n",
    "    # Unless they are actual words (e.g., \"I\" versus \"s\" or \"t\"), they won't be removed in stopping process.\n",
    "    words = [word for word in words if len(word) >= 3]\n",
    "\n",
    "    return words  # return list object of string words\n",
    "\n",
    "def xml_parser(stop_words: list, xml_path: str) -> Rcv1Doc:\n",
    "    \"\"\"Parse a single XML file, process text, and return an Rcv1Doc object with term frequencies.\"\"\"\n",
    "    \n",
    "    # Type check to ensure stop_words is a list of str\n",
    "    if not isinstance(stop_words, list) or not all(isinstance(word, str) for word in stop_words):\n",
    "        raise TypeError(\"stop_words: must be a list of strings.\")\n",
    "    \n",
    "    # Type check to ensure xml_path is a str\n",
    "    if not isinstance(xml_path, str):\n",
    "        raise TypeError(\"xml_path: value must be a str.\")\n",
    "    \n",
    "    # Check if provided xml_path is a valid xml file, raise AttributeError if it is not\n",
    "    if not ((os.path.isfile(xml_path)) and (xml_path.lower().endswith(\".xml\"))):\n",
    "        raise AttributeError(f\"\"\"xml_path: '{xml_path}' is not a valid xml file.\"\"\")\n",
    "        # NOTE: check is included here for targeting single xml (wheras parse_rcv1v2() executes this check in loop)\n",
    "\n",
    "    # DOCUMENT PARSING - recognition of the content and structure of text documents\n",
    "    # Open file in read mode\n",
    "    with open(xml_path, 'r') as file:\n",
    "        xml = file.read()  # read xml in given file\n",
    "\n",
    "    text = re.search(r'<text>\\s*((?:<p>.*?</p>\\s*)+)</text>', xml, re.DOTALL)  # find all text within the <text> tag\n",
    "\n",
    "    # If no text found, raise attribute error; else return match group 1\n",
    "    if not text:\n",
    "        raise AttributeError(fr\"\"\"xml_path: '{xml_path}' did not contain any text, see text tag (expect match at '<text>\\s*((?:<p>.*?</p>\\s*)+)</text>' with re.DOTALL).\"\"\") \n",
    "    else:\n",
    "        text = text.group(1)\n",
    "\n",
    "    # Replace HTML entities with their corresponding characters\n",
    "    html_entities = {\"&lt;\": \"<\", \"&gt;\": \">\", \"&amp;\": \"&\", \"&quot;\": \"\\\"\", \"&apos;\": \"'\", \"&nbsp;\": \" \" }\n",
    "    for entity, char in html_entities.items():\n",
    "        text = text.replace(entity, char)\n",
    "    \n",
    "    text = re.sub(r'<.*?>', ' ', text).strip()  # remove any XML tags (p tags in our case)\n",
    "    \n",
    "    # TOKENIZING - forming words from sequence of characters; critically, generating a list of tokens\n",
    "    words = tokenization(text)\n",
    "    \n",
    "    # POSTING - a collection of arbitrary data (including a pointer)\n",
    "    item_id = re.search(r'<newsitem item_id=\"(\\d+)\"', xml)  # POINTER - a unique identifier of a document (item_id attribute from newsitem element in this case)\n",
    "\n",
    "    if not item_id:\n",
    "        # If no item_id found, raise attribute error\n",
    "        raise AttributeError(f\"\"\"xml_path: '{xml_path}' did not contain pointer, see item_id attribute in newsitem tag (expect match at '<newsitem item_id=\"(\\\\d+)\"').\"\"\") \n",
    "    else:\n",
    "        item_id = item_id.group(1)  # otherwise, take group 1 of regex (just the \\d+ match component)\n",
    "        \n",
    "    document = Rcv1Doc(item_id)  # initialise Rcv1Doc object with the pointer (item_id)\n",
    "\n",
    "    # STOPPING - removing stop (function) words from the text being analysed; have little meaning on their own\n",
    "    words = [word for word in words if word not in stop_words]   \n",
    "    \n",
    "    # STEMMING - reducing words to their word stem, base or root form (remove morphological variations)\n",
    "    stemmer = nltk.stem.PorterStemmer()  # Porter Stemmer: efficient for information retrieval and text processing tasks – though can often create non-words in favour of faster speeds\n",
    "    words = [stemmer.stem(word) for word in words] \n",
    "    \n",
    "    # Iterate over each stemmed word\n",
    "    for stemmed_word in words:\n",
    "        document.add_term(stemmed_word)  # use method add_term to update the Rcv1Doc object (our arbitrary data)          \n",
    "\n",
    "    return document  # return the Rcv1Doc object\n",
    "\n",
    "def parse_rcv1v2(stop_words: list, input_path: str) -> Rcv1Coll:\n",
    "    \"\"\"Parse XML documents in a directory, filter stop words, and return a collection of Rcv1Doc objects.\"\"\"\n",
    "    \n",
    "    # Type check to ensure stop_words is a list of str\n",
    "    if not isinstance(stop_words, list) or not all(isinstance(word, str) for word in stop_words):\n",
    "        raise TypeError(\"stop_words: must be a list of strings.\")\n",
    "    \n",
    "    # Type check to ensure input_path is a str\n",
    "    if not isinstance(input_path, str):\n",
    "        raise TypeError(\"input_path: value must be a str.\")\n",
    "    \n",
    "    # NOTE: need to do attribute check to see if input_path exists\n",
    "\n",
    "    collection = Rcv1Coll()  # initialise Rcv1Coll object (collection of Rcv1Doc objects)\n",
    "    \n",
    "    # Iterate through files in directory\n",
    "    for xml_file in os.listdir(input_path):\n",
    "        xml_path = os.path.join(input_path, xml_file)  # build path to files\n",
    "        if ((os.path.isfile(xml_path)) and (xml_path.lower().endswith(\".xml\"))):\n",
    "            doc = xml_parser(stop_words, xml_path)  # parse xml with xml_parser function\n",
    "            collection.add_doc(doc)  # use method add_doc to update the Rcv1Coll object\n",
    "\n",
    "    # If no xmls parsed (i.e., collection length is 0), raise attribute error\n",
    "    if len(collection.docs) == 0:\n",
    "        raise AttributeError(f\"\"\"input_path: '{input_path}' did not contain any valid xml files.\"\"\")\n",
    "\n",
    "    return collection  # return the Rcv1Coll object\n",
    "\n",
    "def parse_query(query: str, stop_words: list) -> dict:\n",
    "    \"\"\"Tokenize an input query, remove stop words, and return a dictionary of remaining word frequencies.\"\"\"\n",
    "\n",
    "    # Type check to ensure stop_words is a list of str\n",
    "    if not isinstance(stop_words, list) or not all(isinstance(word, str) for word in stop_words):\n",
    "        raise TypeError(\"stop_words: must be a list of strings.\")\n",
    "    \n",
    "    # Type check to ensure query is a str\n",
    "    if not isinstance(query, str):\n",
    "        raise TypeError(\"query: value must be a string.\")\n",
    "    \n",
    "    # TOKENIZING - forming words from sequence of characters; critically, generating a list of tokens\n",
    "    words = tokenization(query)\n",
    "    \n",
    "    # STOPPING - removing stop (function) words from the text being analysed; have little meaning on their own\n",
    "    words = [word for word in words if word not in stop_words]   \n",
    "    \n",
    "    # STEMMING - reducing words to their word stem, base or root form (remove morphological variations)\n",
    "    stemmer = nltk.stem.PorterStemmer()  # Porter Stemmer: efficient for information retrieval and text processing tasks – though can often create non-words in favour of faster speeds\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Constrcut term:frequency dictionary by counting instances of each word (more efficient than for loop + if/else)\n",
    "    query_term_frequency = {stemmed_word: words.count(stemmed_word) for stemmed_word in set(words)}\n",
    "\n",
    "    return query_term_frequency  # return the dictionary containing word frequencies\n",
    "\n",
    "def parse_queries(file_path):\n",
    "    # Type check to ensure the file_path is a string\n",
    "    if not isinstance(file_path, str):\n",
    "        raise TypeError(\"file_path: value must be a string.\")\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "\n",
    "    # Define regex patterns to match each field\n",
    "    num_pattern = re.compile(r'<num>\\s*Number:\\s*(\\w+)', re.MULTILINE)\n",
    "    title_pattern = re.compile(r'<title>([\\w\\s,.-]*)', re.MULTILINE)\n",
    "    desc_pattern = re.compile(r'<desc>\\s*Description:\\s*(.*?)\\n\\n', re.DOTALL)\n",
    "    narr_pattern = re.compile(r'<narr>\\s*Narrative:\\s*(.*?)\\n\\n', re.DOTALL)\n",
    "\n",
    "    # Extract data using regex patterns\n",
    "    nums = num_pattern.findall(data)\n",
    "    titles = title_pattern.findall(data)\n",
    "    descriptions = desc_pattern.findall(data)\n",
    "    narratives = narr_pattern.findall(data)\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    query_frame = pd.DataFrame({\n",
    "        'Number': nums,\n",
    "        'Title': [title.strip() for title in titles],\n",
    "        'Description': [desc.strip() for desc in descriptions],\n",
    "        'Narrative': [narr.strip() for narr in narratives]\n",
    "    })\n",
    "\n",
    "    return query_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'the50Queries.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mparse_queries\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mthe50Queries.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 295\u001b[0m, in \u001b[0;36mparse_queries\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(file_path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_path: value must be a string.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m    296\u001b[0m     data \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    298\u001b[0m \u001b[38;5;66;03m# Define regex patterns to match each field\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'the50Queries.txt'"
     ]
    }
   ],
   "source": [
    "parse_queries('the50Queries.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Design a BM25-based IR model (**BM25**) that ranks documents in each data collection using the corresponding topic (query) for all 50 data collections.\n",
    "\n",
    "\n",
    "**Inputs:** 50 long queries (topics) in *the50Queries.txt* and the corresponding 50 data collections (*Data_C101, Data_C102, …, Data_C150*).\n",
    "\n",
    "\n",
    "**Output:** 50 ranked document files (e.g., for Query *R107*, the output file name is “BM25_R107Ranking.dat”) for all 50 data collections and save them in the folder “RankingOutputs”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each long query (topic) $Q$, you need to use the following equation to calculate a score for each document $D$ in the corresponding data collection (dataset):\n",
    "\n",
    "$\\sum_{i \\in Q} \\log_{10}\\frac{(r_i + 0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)}\\cdot\\frac{(k_1+1)f_i}{K+f_i}\\cdot\\frac{(k_2+1)qf_i}{k_2+qf_i}$\n",
    "\n",
    "- $Q$ is the title of the long query, \n",
    "- $k_1 = 1.2$\n",
    "- $k_2=500$\n",
    "- $b = 0.75$\n",
    "- $dl = len(D)$\n",
    "- $avdl$ is the average length of a document in the dataset. \n",
    "- $K = k1\\cdot((1-b) + b\\cdot dl /avdl)$\n",
    "- The ***base of the log function is 10***. \n",
    "\n",
    "Note that *BM25 values can be negative*, and you may need to update the above equation to produce non-negative values but keep the resulting documents in the same rank order.\n",
    "\n",
    "**Formally describe your design for BM25** in an algorithm to **rank documents in each data collection *using corresponding query* (topic) ***for all 50 data collections*****. When you use the BM25 score to rank the documents of each data collection, you also need to **answer what the query feature function and document feature function are**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bm25(collection: Rcv1Coll, df: dict, q: dict) -> dict:\n",
    "    \"\"\"\n",
    "    BM25 ranking function for a collection of documents and a given query.\n",
    "    Generates a score for a given documents term:frequency set.\n",
    "    Incorporates term frequency (TF) and inverse document frequency (IDF) factors. \n",
    "    It accounts for term frequency saturation as well as document length bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Type check to ensure coll is a Rcv1Coll\n",
    "    if not isinstance(collection, Rcv1Coll):\n",
    "        raise TypeError(\"collection: must be a Rcv1Coll object.\")\n",
    "    \n",
    "    # If no collection contains no docs, raise attribute error\n",
    "    if len(collection.docs) == 0:\n",
    "        raise AttributeError(\"Rcv1collection: object contains no documents (Rcv1Doc objects).\")\n",
    "    \n",
    "    # Type check to ensure q is a dict\n",
    "    if not isinstance(q, dict):\n",
    "        raise TypeError(\"q: must be a dict object.\")\n",
    "    \n",
    "    # Type check to ensure df is a dict\n",
    "    if not isinstance(df, dict):\n",
    "        raise TypeError(\"df: must be a dict object.\")\n",
    "    \n",
    "    R = 0  # number of relevant documents for this query; predefined by task\n",
    "    r_i = 0  # number of relevant documents containing query term i; predefined by task\n",
    "    N = len(collection.docs)  # total number of documents in the collection\n",
    "    \n",
    "    # Setting parameters based on TextREtrieval Conference (TREC) recommendations\n",
    "    k_1 = 1.2  # Controls non-linear term frequency normalization (saturation).\n",
    "    k_2 = 500  # Controls non-linear term frequency normalization for query terms.\n",
    "    b = 0.75  # Controls to what degree document length normalizes tf values.\n",
    "\n",
    "    # Calculate the average document length across the entire collection.\n",
    "    total_doc_len = sum(doc.doc_len for doc in collection.docs.values())  # sum all document lengths\n",
    "    doc_count = len(collection.docs)  # calculate the number of documents in collection\n",
    "    mean_doc_len = total_doc_len / doc_count  # calculate the average document length\n",
    "\n",
    "    # Initialize doc_score\n",
    "    doc_scores = {}\n",
    "\n",
    "    # Loop through each term in the query.\n",
    "    for query_term, query_frequency in q.items():\n",
    "        for doc_ID, doc in collection.docs.items():\n",
    "            doc_scores[doc_ID] = 0  # initialize doc_score\n",
    "\n",
    "            n_i = collection.term_doc_count.get(query_term, 0)  # the number of documents containing term i (0 if not present).\n",
    "            idf_component = ((r_i + 0.5)/(R - r_i + 0.5)) / ((n_i - r_i+0.5) / (N - n_i-R + r_i + 0.5))  # calculate the inverse document frequency component.\n",
    "\n",
    "            for document_term, document_term_frequency in doc.terms.items():\n",
    "                doc_len = sum(frequency for frequency in doc.terms.values())  # get document length (sum of all terms)\n",
    "                if document_term in q.keys():\n",
    "                    K = k_1 * ((1 - b) + b * doc_len / mean_doc_len)  # calculation of K, which normalizes the term frequency.\n",
    "                    tf_component = (((k_1 + 1 ) * document_term_frequency) / (K + document_term_frequency)) * (((k_2 + 1) * query_frequency) / (k_2 + query_frequency))  # calculate the term frequency component for the document term.\n",
    "                    doc_scores[doc_ID] += math.log10(idf_component * tf_component)  # update the document's score with the product of the IDF and TF components.\n",
    "\n",
    "    # Return the document score\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Jelinek-Mercer Lanugage Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Design a Jelinek-Mercer based Language Model (**JM_LM**) that ranks documents in each data collection using the corresponding topic (query) for all 50 data collections.\n",
    "\n",
    "\n",
    "**Inputs:** 50 long queries (topics) in *the50Queries.txt* and the corresponding 50 data collections (*Data_C101, Data_C102, …, Data_C150*).\n",
    "\n",
    "\n",
    "**Output:** 50 ranked document files (e.g., for Query *R107*, the output file name is “JM_LM_R107Ranking.dat”) for all 50 data collections and save them in the folder RankingOutputs”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each long query (topic) $R_x$, you need to use the following equation to calculate a conditional probability for each document $D$ in the corresponding data collection (dataset):\n",
    "\n",
    "\n",
    "$p(R_x|D)=\\Pi_{i=1}^n ((1-\\lambda)\\cdot\\frac{f_{q_i,D}}{|D|}+\\lambda\\cdot\\frac{c_{q_i}}{|Data\\_C_x|})$\n",
    "\n",
    "- $f_{q_i,D}$ is the number of times query word qi occurs in document $D$\n",
    "- $|D|$ is the number of word occurrences in $D$\n",
    "- $c_{q_i}$ is the number of times query word qi occurs in the data collection $Data_Cx$\n",
    "- $|Data\\_C_x|$ is the total number of word occurrences in data collection $Data\\_C_x$\n",
    "- `λ = 0.4`\n",
    "\n",
    "**Formally describe your design for JM_LM** in an algorithm to **rank documents in each data collection *using corresponding query* (topic) ***for all 50 data collections*****. When you use the probabilities to rank the documents of each data collection, you also need to **answer what the query feature function and document feature function are**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Pseudo-Relevance Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Based on the knowledge you gained from this unit, design a pseudo-relevance model (My_PRM) to rank documents in each data collection using the corresponding topic (query) for all 50 data collections.\n",
    "\n",
    "\n",
    "**Inputs:** 50 long queries (topics) in the50Queries.txt and the corresponding 50 data collections (Data_C101, Data_C102, …, Data_C150).\n",
    "\n",
    "\n",
    "**Output:** 50 ranked document files (e.g., for Query R107, the output file name is “My_PRM_R107Ranking.dat”) for all 50 data collections and save them in the folder RankingOutputs”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formally describe your design for My_PRM** in an algorithm to **rank documents in each data collection *using corresponding query* (topic) ***for all 50 data collections*****.Your *approach should be generic*; that means it is feasible to be used for other topics (queries). You also need to **discuss the differences between My_PRM and the other two models (BM25 and JM_LM)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Use Python to implement three models: `BM25`, `JM_LM`, and `My_PRM`, and **test them on the given 50 data collections for the corresponding 50 queries (topics)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Python programs to implement these three models. You can use a .py file (or a .ipynb file) for each model.\n",
    "\n",
    "\n",
    "For each long query, your python programs will produce ranked results and save them into .dat files. For example, for query R107, you can save the ranked results of three models into “BM25_R107Ranking.dat”, “JM_LM_R107Ranking.dat”, and “My_PRM_R107Ranking.dat”, respectively by using the following format:\n",
    "- The first column is the document id (the itemid in the corresponding XML document)\n",
    "- The second column is the document score (or probability).\n",
    "\n",
    "**Describe:** \n",
    "- Python packages or modules (or any open-source software) you used\n",
    "- The data structures used to represent a single document and a set of documents for each model (you can use different data structures for different models).\n",
    "\n",
    "\n",
    "You also need to **test the three models on the given 50 data collections for the 50 queries (topics) by *printing out the top 15 documents* for each data collection (in descending order)**. The **output will also be put in the appendix of your final report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Use three effectiveness measures to evaluate the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you need to **use the relevance judgments (EvaluationBenchmark.zip)** to **compare with the ranking outputs in the folder of “RankingOutputs” for the selected effectiveness metric** for the three models.\n",
    "\n",
    "\n",
    "You need to use the following three different effectiveness measures to evaluate the document ranking results you saved in the folder “RankingOutputs”:\n",
    "1) Average precision (and MAP)\n",
    "2) Precision@10 (and their average)\n",
    "3) Discounted cumulative gain at rank position 10 ($p = 10$), $DCG_{10}$ (and their average):  \n",
    "    $DCG_p=rel_i+\\sum_{i=2}^p\\frac{rel_i}{log_2(i)}$  \n",
    "        $rel_i=1$ if the document at position $i$ is releveant; otherwise, it is 0.\n",
    "\n",
    "Evaluation results can be summarized in tables or graphs. Examples are provided in the sepcification sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Recommend a model based on significance test and your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to conduct a significance test to compare models. You can choose a t-test to perform a significance test on the evaluation results (e.g., in Tables 1, 2 and 3). \n",
    "\n",
    "You can compare models between:\n",
    "- **BM25** and **JM_LM**\n",
    "- **BM25** and **My_PRM**\n",
    "- **JM_LM** and **My_PRM**\n",
    "\n",
    "Based on $t$-test results ($p$-value and $t$-statistic), you can recommend a model (You ***want the proposed \"My_RPM\" to be the best because it is your own model***). You can perform the $t$-test using a single effectiveness measure or multiple measures. Generally, using more effectiveness measures provides stronger evidence against the null hypothesis. Note that if the $t$-test is unsatisfactory, you can use the evaluation results to refine **My_PRM** mode. For example, you can adjust parameter settings or update your design and implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
