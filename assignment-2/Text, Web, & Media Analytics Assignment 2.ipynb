{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text, Web, & Media Analytics Assignment 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: ORDER ME PLEASE 🙇‍♂️\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import regex as re\n",
    "import string\n",
    "import math\n",
    "import csv\n",
    "\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class for Bag-of-Word representation\n",
    "class bow_document:\n",
    "    def __init__(self, item_id: str):\n",
    "        # Type check to ensure object is initialised correctly\n",
    "        if not isinstance(item_id, str):\n",
    "            raise TypeError(\"item_id: value must be a string.\")\n",
    "            # Technically could work with str or int indexing (for key in collection),\n",
    "            # using *only* str ensures no double-up of pointers\n",
    "            # (e.g. item_id '1' vs item_id 1)\n",
    "\n",
    "        self.doc_id = item_id  # assigning doc_id from 'item_id'\n",
    "        self.terms = {}  # dictionary for terms and their frequencies\n",
    "        self._doc_len = 0  # document length, private attribute\n",
    "\n",
    "    def add_term(self, term: str):\n",
    "        \"\"\"Add a term to the document or update its frequency if it already exists.\"\"\"\n",
    "        \n",
    "        # Type check to ensure term is a str\n",
    "        if not isinstance(term, str):\n",
    "            raise TypeError(\"term: value must be a string.\")\n",
    "        \n",
    "        self.doc_len += 1  # extend doc_len\n",
    "\n",
    "        if term in self.terms:\n",
    "            self.terms[term] += 1  # add frequency if the term exists\n",
    "        else:\n",
    "            self.terms[term] = 1  # if it doesn't exist, add it (setting frequency to 1)\n",
    "        \n",
    "    def get_doc_id(self) -> str:\n",
    "        \"\"\"Return the document ID.\"\"\"\n",
    "        return self.doc_id\n",
    "    \n",
    "    def get_term_list(self, sorted_by_freq: bool = None) -> dict:\n",
    "        \"\"\"\n",
    "        Return a list of terms occurring in the document, optionally sorted by their frequency.\n",
    "        If sorted_by_freq is True, the terms are returned sorted by their frequency in descending order.\n",
    "        If sorted_by_freq is False or None (default), the terms are returned in arbitrary order.\n",
    "        \"\"\"\n",
    "\n",
    "        # Type check to ensure sorted_by_freq is either None or a boolean\n",
    "        if not isinstance(sorted_by_freq, (bool, type(None))):\n",
    "            raise TypeError(\"sorted_by_freq: must be a boolean or None.\")\n",
    "\n",
    "        if sorted_by_freq:\n",
    "            # If sorted_by_freq is True\n",
    "            sorted_terms = sorted(self.terms.items(), key=lambda word: word[1], reverse=sorted_by_freq)  # generate a sorted list of terms by frequency\n",
    "            return {term: freq for term, freq in sorted_terms}  # return key:value pairs based on sorted terms\n",
    "        else:\n",
    "            # If sorted_by_freq is False or None, return the terms as is (i.e., unsorted and as they are added in)\n",
    "            return self.terms\n",
    "        \n",
    "    def get_bag_of_words(self, sorted_by_freq: bool = None) -> str:\n",
    "        \"\"\"Return full bag-of-words representation for bow_document object, including; doc_id, term_count, doc_len, and terms.\"\"\"\n",
    "        \n",
    "        # Type check to ensure sorted_by_freq is either None or a boolean\n",
    "        if not isinstance(sorted_by_freq, (bool, type(None))):\n",
    "            raise TypeError(\"sorted_by_freq: must be a boolean or None.\")\n",
    "\n",
    "        # Defining formatted string for bag-of-word representation\n",
    "        bag_of_words = f\"\"\"doc_id='{self.doc_id}',term_count={len(self.get_term_list())},doc_len={self.doc_len},terms={self.get_term_list(sorted_by_freq)}\"\"\"\n",
    "\n",
    "        return bag_of_words  # return BOW representation; this kind of data can be stored and \"unpacked\" easily\n",
    "    \n",
    "    @property  # accessor (get) method for doc_len\n",
    "    def doc_len(self) -> int:\n",
    "        \"\"\"The doc_len property getter method.\"\"\"\n",
    "        return self._doc_len\n",
    "\n",
    "    @doc_len.setter  # mutator (setter) method for doc_len\n",
    "    def doc_len(self, value: int):\n",
    "        \"\"\"The doc_len property setter method.\"\"\"\n",
    "        if not isinstance(value, int):\n",
    "            raise TypeError(\"doc_len: must be an int.\")\n",
    "        if value < 0:\n",
    "            raise ValueError(\"doc_len: must not be negative.\")\n",
    "        \n",
    "        self._doc_len = value\n",
    "\n",
    "# Define the class for collection of bow_document objects\n",
    "class bow_document_collection:\n",
    "    def __init__(self):\n",
    "        self.docs = {}  # initialise dictionary to hold collection (dict) of doc_id:bow_document\n",
    "\n",
    "        self.term_doc_count = {}  # initialise dictionary to track the number of documents each term appears in\n",
    "\n",
    "    # Method to add a doc (bow_document object)\n",
    "    def add_doc(self, doc: bow_document):\n",
    "        \"\"\"Add bow_document object to the collection, using doc_id as the key, and update the inverted index.\"\"\"\n",
    "\n",
    "        # Type check to ensure doc is a bow_document object\n",
    "        if not isinstance(doc, bow_document):\n",
    "            raise TypeError(\"doc: must be an instance of bow_document.\")\n",
    "        \n",
    "        # Add to the docs dict; key as doc_id and value as bow_document object (doc_id:bow_document)\n",
    "        self.docs[doc.get_doc_id()] = doc\n",
    "\n",
    "        # Update term document count for each term\n",
    "        for term in doc.terms:\n",
    "            if term in self.term_doc_count:\n",
    "                self.term_doc_count[term] += 1  # add one if the term exists in the corpus dictionary\n",
    "            else:\n",
    "                self.term_doc_count[term] = 1  # if it does not exist in the corpus dictionary, initialise by setting to 1\n",
    "    \n",
    "    def get_collection_ids(self) -> str:\n",
    "        \"\"\"Return list of document IDs present in the collection.\"\"\"\n",
    "\n",
    "        # Type check to ensure doc_id is a string\n",
    "        if not len(self.docs) > 0:\n",
    "            raise AttributeError(\"bow_document_collection object is empty, no IDs to return.\")  # Corrected to match the check\n",
    "        \n",
    "        doc_ids_str = \"'\" + \"', '\".join(self.docs.keys()) + \"'\"  # create a string that lists doc_ids\n",
    "\n",
    "        collection_ids = f\"bow_document_collection(doc_ids: {doc_ids_str})\"  # format the return variable\n",
    "\n",
    "        return collection_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stop_word_parser(stop_word_path: str) -> list:\n",
    "    \"\"\"Parse defined list of stop words (assumes txt file with words delimited with ',').\"\"\"\n",
    "\n",
    "    # Type check to ensure stop_word_path is a str\n",
    "    if not isinstance(stop_word_path, str):\n",
    "        raise TypeError(\"stop_word_path: value must be a str.\")\n",
    "    \n",
    "    # NOTE: need attribute check the path exists\n",
    "\n",
    "    # Open file in read mode\n",
    "    with open(stop_word_path, 'r') as file:\n",
    "        stop_words = file.read()  # read text in given file into stop_words\n",
    "\n",
    "    # We know what the format is ahead of time, so not a lot of processing needed;\n",
    "    # i.e., assumes we don't need to make something more robust and that we're using the same txt\n",
    "    stop_words = stop_words.lower().split(\",\")  # tokenize stop_words; delimited with ','\n",
    "    stop_words = list(set(stop_words))  # reduce stop_words to uniques\n",
    "    \n",
    "    return stop_words  # return stop_words as a list object\n",
    "\n",
    "def tokenization(words: str) -> list:\n",
    "    \"\"\"Tokenize input text by removing line breaks, numbers, punctuation, normalizing whitespace, stripping leading/trailing spaces, and splitting into lowercased words.\"\"\"\n",
    "\n",
    "    # Type check to ensure words is a str\n",
    "    if not isinstance(words, str):\n",
    "        raise TypeError(\"words: value must be a str.\")\n",
    "\n",
    "    words = words.replace(\"\\n\", \"\")  # don't want line breaks to contribute\n",
    "    words = re.sub(r'\\d', '', words)  # not interested in numbers for this particular task, remove\n",
    "    words = re.sub(f'[{re.escape(string.punctuation)}]', ' ', words)  # not interested in punctuation, remove\n",
    "    words = re.sub(r'\\s+|\\t+|\\v+|\\n+|\\r+|\\f+', ' ', words).strip()  # standardise the whitespaces, remove leading/trailing whitespace\n",
    "    words = words.lower()  # standardise words as lower\n",
    "    words = words.split()  # tokenize, deftault split on space\n",
    "\n",
    "    # Filter out small words; can be important in some queries, usually in combinations, opting not to handle for simplicity.\n",
    "    # For example, with no discrete management of apostrophes (indicating contractions or posession) aside from replacement \n",
    "    # of punctuation with a single space, we will get the following: \"Amelia's\" → [\"Amelia\", \"s\"] → [\"Amelia\"].\n",
    "    # Unless they are actual words (e.g., \"I\" versus \"s\" or \"t\"), they won't be removed in stopping process.\n",
    "    words = [word for word in words if len(word) >= 3]\n",
    "\n",
    "    return words  # return list object of string words\n",
    "\n",
    "def xml_parser(stop_words: list, xml_path: str) -> bow_document:\n",
    "    \"\"\"Parse a single XML file, process text, and return an bow_document object with term frequencies.\"\"\"\n",
    "    \n",
    "    # Type check to ensure stop_words is a list of str\n",
    "    if not isinstance(stop_words, list) or not all(isinstance(word, str) for word in stop_words):\n",
    "        raise TypeError(\"stop_words: must be a list of strings.\")\n",
    "    \n",
    "    # Type check to ensure xml_path is a str\n",
    "    if not isinstance(xml_path, str):\n",
    "        raise TypeError(\"xml_path: value must be a str.\")\n",
    "    \n",
    "    # Check if provided xml_path is a valid xml file, raise AttributeError if it is not\n",
    "    if not ((os.path.isfile(xml_path)) and (xml_path.lower().endswith(\".xml\"))):\n",
    "        raise AttributeError(f\"\"\"xml_path: '{xml_path}' is not a valid xml file.\"\"\")\n",
    "        # NOTE: check is included here for targeting single xml (wheras parse_rcv1v2() executes this check in loop)\n",
    "\n",
    "    # DOCUMENT PARSING - recognition of the content and structure of text documents\n",
    "    # Open file in read mode\n",
    "    with open(xml_path, 'r') as file:\n",
    "        xml = file.read()  # read xml in given file\n",
    "\n",
    "    text = re.search(r'<text>\\s*((?:<p>.*?</p>\\s*)+)</text>', xml, re.DOTALL)  # find all text within the <text> tag\n",
    "\n",
    "    # If no text found, raise attribute error; else return match group 1\n",
    "    if not text:\n",
    "        raise AttributeError(fr\"\"\"xml_path: '{xml_path}' did not contain any text, see text tag (expect match at '<text>\\s*((?:<p>.*?</p>\\s*)+)</text>' with re.DOTALL).\"\"\") \n",
    "    else:\n",
    "        text = text.group(1)\n",
    "\n",
    "    # Replace HTML entities with their corresponding characters\n",
    "    html_entities = {\"&lt;\": \"<\", \"&gt;\": \">\", \"&amp;\": \"&\", \"&quot;\": \"\\\"\", \"&apos;\": \"'\", \"&nbsp;\": \" \" }\n",
    "    for entity, char in html_entities.items():\n",
    "        text = text.replace(entity, char)\n",
    "    \n",
    "    text = re.sub(r'<.*?>', ' ', text).strip()  # remove any XML tags (p tags in our case)\n",
    "    \n",
    "    # TOKENIZING - forming words from sequence of characters; critically, generating a list of tokens\n",
    "    words = tokenization(text)\n",
    "    \n",
    "    # POSTING - a collection of arbitrary data (including a pointer)\n",
    "    item_id = re.search(r'<newsitem itemid=\"(\\d+)\"', xml)  # POINTER - a unique identifier of a document (item_id attribute from newsitem element in this case)\n",
    "\n",
    "    if not item_id:\n",
    "        # If no item_id found, raise attribute error\n",
    "        raise AttributeError(f\"\"\"xml_path: '{xml_path}' did not contain pointer, see item_id attribute in newsitem tag (expect match at '<newsitem itemid=\"(\\\\d+)\"').\"\"\") \n",
    "    else:\n",
    "        item_id = item_id.group(1)  # otherwise, take group 1 of regex (just the \\d+ match component)\n",
    "        \n",
    "    document = bow_document(item_id)  # initialise bow_document object with the pointer (item_id)\n",
    "\n",
    "    # STOPPING - removing stop (function) words from the text being analysed; have little meaning on their own\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # STEMMING - reducing words to their word stem, base or root form (remove morphological variations)\n",
    "    stemmer = nltk.stem.PorterStemmer()  # Porter Stemmer: efficient for information retrieval and text processing tasks – can often create non-words in favour of faster speeds\n",
    "    words = [stemmer.stem(word) for word in words] \n",
    "    \n",
    "    # Iterate over each stemmed word\n",
    "    for stemmed_word in words:\n",
    "        document.add_term(stemmed_word)  # use method add_term to update the bow_document object (our arbitrary data)          \n",
    "\n",
    "    return document  # return the bow_document object\n",
    "\n",
    "def parse_rcv1v2(stop_words: list, input_path: str) -> bow_document_collection:\n",
    "    \"\"\"Parse XML documents in a directory, filter stop words, and return a collection of bow_document objects.\"\"\"\n",
    "    \n",
    "    # Type check to ensure stop_words is a list of str\n",
    "    if not isinstance(stop_words, list) or not all(isinstance(word, str) for word in stop_words):\n",
    "        raise TypeError(\"stop_words: must be a list of strings.\")\n",
    "    \n",
    "    # Type check to ensure input_path is a str\n",
    "    if not isinstance(input_path, str):\n",
    "        raise TypeError(\"input_path: value must be a str.\")\n",
    "    \n",
    "    # NOTE: need to do attribute check to see if input_path exists\n",
    "\n",
    "    collection = bow_document_collection()  # initialise bow_document_collection object (collection of bow_document objects)\n",
    "    \n",
    "    # Iterate through files in directory\n",
    "    for xml_file in os.listdir(input_path):\n",
    "        xml_path = os.path.join(input_path, xml_file)  # build path to files\n",
    "        if ((os.path.isfile(xml_path)) and (xml_path.lower().endswith(\".xml\"))):\n",
    "            doc = xml_parser(stop_words, xml_path)  # parse xml with xml_parser function\n",
    "            collection.add_doc(doc)  # use method add_doc to update the bow_document_collection object\n",
    "\n",
    "    # If no xmls parsed (i.e., collection length is 0), raise attribute error\n",
    "    if len(collection.docs) == 0:\n",
    "        raise AttributeError(f\"\"\"input_path: '{input_path}' did not contain any valid xml files.\"\"\")\n",
    "\n",
    "    return collection  # return the bow_document_collection object\n",
    "\n",
    "def parse_query(query: str, stop_words: list) -> dict:\n",
    "    \"\"\"Tokenize an input query, remove stop words, and return a dictionary of remaining word frequencies.\"\"\"\n",
    "\n",
    "    # Type check to ensure stop_words is a list of str\n",
    "    if not isinstance(stop_words, list) or not all(isinstance(word, str) for word in stop_words):\n",
    "        raise TypeError(\"stop_words: must be a list of strings.\")\n",
    "    \n",
    "    # Type check to ensure query is a str\n",
    "    if not isinstance(query, str):\n",
    "        raise TypeError(\"query: value must be a string.\")\n",
    "    \n",
    "    # TOKENIZING - forming words from sequence of characters; critically, generating a list of tokens\n",
    "    words = tokenization(query)\n",
    "    \n",
    "    # STOPPING - removing stop (function) words from the text being analysed; have little meaning on their own\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # STEMMING - reducing words to their word stem, base or root form (remove morphological variations)\n",
    "    stemmer = nltk.stem.PorterStemmer()  # Porter Stemmer: efficient for information retrieval and text processing tasks – though can often create non-words in favour of faster speeds\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Constrcut term:frequency dictionary by counting instances of each word (more efficient than for loop + if/else)\n",
    "    query_term_frequency = {stemmed_word: words.count(stemmed_word) for stemmed_word in set(words)}\n",
    "\n",
    "    return query_term_frequency  # return the dictionary containing word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_queries(file_path: str) -> pd.DataFrame:\n",
    "    # Type check to ensure the file_path is a string\n",
    "    if not isinstance(file_path, str):\n",
    "        raise TypeError(\"file_path: value must be a string.\")\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        data = file.read()\n",
    "    \n",
    "    # Define regex pattern to split queries\n",
    "    query_pattern = re.compile(r'<Query>(.*?)</Query>', re.DOTALL)\n",
    "    queries = query_pattern.findall(data)\n",
    "    \n",
    "    # Initialize lists for storing parsed data\n",
    "    nums, titles, descriptions, narratives = [], [], [], []\n",
    "    \n",
    "    # Define regex patterns to extract individual fields\n",
    "    num_pattern = re.compile(r'<num>\\s*Number:\\s*R(\\w+)', re.MULTILINE)\n",
    "    title_pattern = re.compile(r'<title>([\\w\\s,.-]*)', re.MULTILINE)\n",
    "    desc_pattern = re.compile(r'<desc>\\s*Description:\\s*(.*?)\\n\\n', re.DOTALL)\n",
    "    narr_pattern = re.compile(r'<narr>\\s*Narrative:\\s*(.*?)\\n\\n', re.DOTALL)\n",
    "    \n",
    "    for query in queries:\n",
    "        # Extract data using regex patterns\n",
    "        num_match = num_pattern.search(query)\n",
    "        title_match = title_pattern.search(query)\n",
    "        desc_match = desc_pattern.search(query)\n",
    "        narr_match = narr_pattern.search(query)\n",
    "        \n",
    "        nums.append(num_match.group(1) if num_match else pd.NA)\n",
    "        titles.append(title_match.group(1).strip() if title_match else pd.NA)\n",
    "        descriptions.append(desc_match.group(1).strip() if desc_match else pd.NA)\n",
    "        narratives.append(narr_match.group(1).strip() if narr_match else pd.NA)\n",
    "\n",
    "    # Create a pandas DataFrame\n",
    "    query_frame = pd.DataFrame({\n",
    "        'Number': nums,\n",
    "        'Title': titles,\n",
    "        'Description': descriptions,\n",
    "        'Narrative': narratives\n",
    "    })\n",
    "    \n",
    "    return query_frame\n",
    "\n",
    "def write_scores_to_file(scores: dict, filename: str):\n",
    "    \"\"\"\n",
    "    Write the scores dictionary to a .dat file.\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(scores, dict):\n",
    "        raise TypeError(\"scores: value must be a dictionary.\")\n",
    "    \n",
    "    if not all((isinstance(doc_id, str)) and (isinstance(score, (int, float))) for doc_id, score in scores.items()):\n",
    "        raise ValueError(\"scores: dictionary must consist of string keys (for documents) and int/float values (for document scores).\")\n",
    "\n",
    "    if not isinstance(filename, str):\n",
    "        raise TypeError(\"filename: value must be a string.\")\n",
    "    \n",
    "    # Combine the directory and filename to form the full path\n",
    "    directory = 'RankingOutputs'\n",
    "    filepath = os.path.join(directory, filename)\n",
    "\n",
    "    # Check if the directory exists, and create it if it doesn't\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    with open(filepath, 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter='\\t')  # Using tab delimiter for .dat format\n",
    "        for doc_id, score in scores.items():\n",
    "            writer.writerow([doc_id, score])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: BM25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Design a BM25-based IR model (**BM25**) that ranks documents in each data collection using the corresponding topic (query) for all 50 data collections.\n",
    "\n",
    "\n",
    "**Inputs:** 50 long queries (topics) in *the50Queries.txt* and the corresponding 50 data collections (*Data_C101, Data_C102, …, Data_C150*).\n",
    "\n",
    "\n",
    "**Output:** 50 ranked document files (e.g., for Query *R107*, the output file name is “BM25_R107Ranking.dat”) for all 50 data collections and save them in the folder “RankingOutputs”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each long query (topic) $Q$, you need to use the following equation to calculate a score for each document $D$ in the corresponding data collection (dataset):\n",
    "\n",
    "$\\sum_{i \\in Q} \\log_{10}(\\frac{(r_i + 0.5)/(R-r_i+0.5)}{(n_i-r_i+0.5)/(N-n_i-R+r_i+0.5)})\\cdot\\frac{(k_1+1)f_i}{K+f_i}\\cdot\\frac{(k_2+1)qf_i}{k_2+qf_i}$\n",
    "\n",
    "- $Q$ is the title of the long query, \n",
    "- $k_1 = 1.2$\n",
    "- $k_2=500$\n",
    "- $b = 0.75$\n",
    "- $dl = len(D)$\n",
    "- $avdl$ is the average length of a document in the dataset. \n",
    "- $K = k1\\cdot((1-b) + b\\cdot dl /avdl)$\n",
    "- The ***base of the log function is 10***. \n",
    "\n",
    "Note that *BM25 values can be negative*, and you may need to update the above equation to produce non-negative values but keep the resulting documents in the same rank order.\n",
    "\n",
    "**Formally describe your design for BM25** in an algorithm to **rank documents in each data collection *using corresponding query* (topic) ***for all 50 data collections*****. When you use the BM25 score to rank the documents of each data collection, you also need to **answer what the query feature function and document feature function are**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BM25(collection: bow_document_collection, query: dict) -> dict:\n",
    "    \"\"\"\n",
    "    BM25 ranking function for a collection of documents and a given query.\n",
    "    Generates a score for a given documents term:frequency set.\n",
    "    Incorporates term frequency (TF) and inverse document frequency (IDF) factors. \n",
    "    It accounts for term frequency saturation as well as document length bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Type check to ensure coll is a bow_document_collection\n",
    "    if not isinstance(collection, bow_document_collection):\n",
    "        raise TypeError(\"collection: must be a bow_document_collection object.\")\n",
    "    \n",
    "    # If no collection contains no documents, raise attribute error\n",
    "    if len(collection.docs) == 0:\n",
    "        raise AttributeError(\"bow_document_collectionection: object contains no documents (Rcv1Doc objects).\")\n",
    "    \n",
    "    # Type check to ensure query is a dict\n",
    "    if not isinstance(query, dict):\n",
    "        raise TypeError(\"query: must be a dict object.\")\n",
    "    \n",
    "    # Setting parameters\n",
    "    k_1 = 1.2  # Controls non-linear term frequency normalization (saturation)\n",
    "    k_2 = 500  # Controls non-linear term frequency normalization for query terms\n",
    "    b = 0.75  # Controls to what degree document length normalizes tf values\n",
    "\n",
    "    N = len(collection.docs)  # total number of documents in the collection\n",
    "    R = 0  # number of relevant documents for this query; predefined by task\n",
    "    r_i = 0  # number of relevant documents containing query term i; predefined by task\n",
    "\n",
    "    # Calculate the average document length across the entire collection\n",
    "    total_corpus_length = sum(doc.doc_len for doc in collection.docs.values())\n",
    "    mean_doc_len = total_corpus_length / N\n",
    "    \n",
    "    doc_scores = {}  # initialize doc_score dictionary to store calculated scores\n",
    "\n",
    "    # Loop through each term in the query.\n",
    "    for query_term, query_frequency in query.items():\n",
    "        n_i = collection.term_doc_count.get(query_term, 0)  # the number of documents containing term i (0 if not present)\n",
    "\n",
    "        # Calculate the inverse document frequency for the term\n",
    "        idf_component = math.log10(((r_i + 0.5)/(R - r_i + 0.5)) / ((n_i - r_i + 0.5) / (N - n_i - R + r_i + 0.5)))\n",
    "        # idf_component = math.log10((N - n_i + 0.5) / (n_i + 0.5))  # NOTE: simplified, need feedback from Slack\n",
    "\n",
    "        # Component measures the rarity of the term across the entire collection; \n",
    "        # term appearing in fewer documents will have a higher IDF, making it more influential.\n",
    "        # Formula ensures that no division by zero occurs by introducing additive smoothing of 0.5 to the numerator and denominator.\n",
    "\n",
    "        for doc_ID, doc in collection.docs.items():\n",
    "            doc_len = doc.doc_len  # document length\n",
    "\n",
    "            K = k_1 * ((1 - b) + b * doc_len / mean_doc_len)  # frequency normaliser\n",
    "            \n",
    "            document_term_frequency = doc.terms.get(query_term, 0)  # query term frequency within the document (0 if not present)\n",
    "            \n",
    "            # Calculate the term frequency normalization for the document term\n",
    "            tf_component = ((k_1 + 1) * document_term_frequency) / (K + document_term_frequency)\n",
    "            # This component adjusts the score based on the frequency of the term in the document.\n",
    "            # The normalisation (denominator) prevents over-emphasis on terms that appear too frequently within a single document.\n",
    "            # `k_1` controls the non-linear term frequency saturation, and `K` adjusts the weight based on document length.\n",
    "\n",
    "            # Calculate the query term frequency normalization\n",
    "            query_component = ((k_2 + 1) * query_frequency) / (k_2 + query_frequency)\n",
    "            # Adjusts the score based on the query term's frequency.\n",
    "            # Denominator prevents over-emphasis on query terms that appear frequently.\n",
    "            \n",
    "            score = idf_component * tf_component * query_component  # determine the score (can be non-negative, clamping used below to adjust)\n",
    "            \n",
    "            if doc_ID not in doc_scores:\n",
    "                doc_scores[doc_ID] = 0  # initialize doc_score if not present\n",
    "            \n",
    "            doc_scores[doc_ID] += max(score, 0)  # update the document's score with the product of the IDF and TF components (clamping non-negatives to 0)\n",
    "    \n",
    "    doc_scores = dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))  # sort the results\n",
    "\n",
    "    # Return the document score\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stop_word_parser('common-english-words.txt')\n",
    "\n",
    "query_frame = parse_queries('the50Queries.txt')\n",
    "query_frame['parsed_titles'] = query_frame['Title'].apply(lambda row: parse_query(row, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_set = {}\n",
    "\n",
    "input_path = 'Data_Collection'\n",
    "for collection_path in os.listdir(input_path):\n",
    "    data_key = collection_path.split('_C', 1)[1]\n",
    "    document_set[data_key] = parse_rcv1v2(stop_words, os.path.join(input_path, collection_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_results = {}\n",
    "\n",
    "for query_key, collection in document_set.items():\n",
    "    query = query_frame.loc[query_frame['Number'] == query_key, 'parsed_titles'].iloc[0]\n",
    "    BM25_results[query_key] = BM25(collection, query)\n",
    "    write_scores_to_file(BM25_results[query_key], f\"BM25_R{query_key}Ranking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Jelinek-Mercer Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Design a Jelinek-Mercer based Language Model (**JM_LM**) that ranks documents in each data collection using the corresponding topic (query) for all 50 data collections.\n",
    "\n",
    "\n",
    "**Inputs:** 50 long queries (topics) in *the50Queries.txt* and the corresponding 50 data collections (*Data_C101, Data_C102, …, Data_C150*).\n",
    "\n",
    "\n",
    "**Output:** 50 ranked document files (e.g., for Query *R107*, the output file name is “JM_LM_R107Ranking.dat”) for all 50 data collections and save them in the folder RankingOutputs”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each long query (topic) $R_x$, you need to use the following equation to calculate a conditional probability for each document $D$ in the corresponding data collection (dataset):\n",
    "\n",
    "\n",
    "$p(R_x|D)=\\Pi_{i=1}^n ((1-\\lambda)\\cdot\\frac{f_{q_i,D}}{|D|}+\\lambda\\cdot\\frac{c_{q_i}}{|C|})$\n",
    "\n",
    "- $f_{q_i,D}$ is the number of times query word $q_i$ occurs in document $D$\n",
    "- $|D|$ is the number of word occurrences in $D$\n",
    "- $c_{q_i}$ is the number of times query word $q_i$ occurs in the data collection $C$\n",
    "- $|C|$ is the total number of word occurrences in data collection $C$\n",
    "- `λ = 0.4`\n",
    "\n",
    "**Formally describe your design for JM_LM** in an algorithm to **rank documents in each data collection *using corresponding query* (topic) ***for all 50 data collections*****. When you use the probabilities to rank the documents of each data collection, you also need to **answer what the query feature function and document feature function are**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def JM_LM(collection, query):\n",
    "    \"\"\"\n",
    "    Calculate the conditional probability of each document given a query using the Jelinek-Mercer smoothing Language Model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Type check to ensure collection is an bow_document_collection object\n",
    "    if not isinstance(collection, bow_document_collection):\n",
    "        raise TypeError(\"collection: must be a bow_document_collection object.\")\n",
    "    \n",
    "    # Check if the collection contains any documents\n",
    "    if len(collection.docs) == 0:\n",
    "        raise AttributeError(\"bow_document_collection: object contains no documents (Rcv1Doc objects).\")\n",
    "    \n",
    "    # Validate that the query is a dictionary\n",
    "    if not isinstance(query, dict):\n",
    "        raise TypeError(\"query: must be a dict object.\")\n",
    "    \n",
    "    # Set lambda parameter for Jelinek-Mercer smoothing\n",
    "    lambda_val = 0.4\n",
    "    \n",
    "    # Calculate the total length of the corpus by summing the lengths of all documents\n",
    "    total_corpus_length = sum(doc.doc_len for doc in collection.docs.values())\n",
    "    \n",
    "    # Initialize an empty dictionary to store the scores for each document\n",
    "    doc_scores = {}\n",
    "\n",
    "    # Iterate through each term in the query\n",
    "    for query_term in query:\n",
    "        # Get the frequency of the query term in the entire collection\n",
    "        c_qi = collection.term_doc_count.get(query_term, 0)\n",
    "        \n",
    "        # Iterate through each document in the collection\n",
    "        for doc_ID, doc in collection.docs.items():\n",
    "            # Get the frequency of the query term in the current document\n",
    "            f_qi_D = doc.terms.get(query_term, 0)\n",
    "\n",
    "            # Calculate the probability of the term occurring in the document\n",
    "            p_doc = (f_qi_D / doc.doc_len) if doc.doc_len > 0 else 0\n",
    "            \n",
    "            # Calculate the probability of the term occurring in the whole collection\n",
    "            p_coll = (c_qi / total_corpus_length) if total_corpus_length > 0 else 0\n",
    "\n",
    "            # Calculate the smoothed score for the term using Jelinek-Mercer smoothing\n",
    "            score = (1 - lambda_val) * p_doc + lambda_val * p_coll\n",
    "\n",
    "            # Initialize the score for the document if not already done\n",
    "            if doc_ID not in doc_scores:\n",
    "                doc_scores[doc_ID] = 1  # Multiplicative identity\n",
    "\n",
    "            # Multiply the score to the cumulative product if it's greater than zero\n",
    "            if score > 0:\n",
    "                doc_scores[doc_ID] *= score\n",
    "\n",
    "    # Sort the documents by their score in descending order and return the sorted dictionary\n",
    "    doc_scores = dict(sorted(doc_scores.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "    # Return the document scores\n",
    "    return doc_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "JM_LM_results = {}\n",
    "\n",
    "for query_key, collection in document_set.items():\n",
    "    query = query_frame.loc[query_frame['Number'] == query_key, 'parsed_titles'].iloc[0]\n",
    "    JM_LM_results[query_key] = JM_LM(collection, query)\n",
    "    write_scores_to_file(JM_LM_results[query_key], f\"JM_LM_R{query_key}Ranking\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Pseudo-Relevance Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Based on the knowledge you gained from this unit, design a pseudo-relevance model (My_PRM) to rank documents in each data collection using the corresponding topic (query) for all 50 data collections.\n",
    "\n",
    "\n",
    "**Inputs:** 50 long queries (topics) in the50Queries.txt and the corresponding 50 data collections (Data_C101, Data_C102, …, Data_C150).\n",
    "\n",
    "\n",
    "**Output:** 50 ranked document files (e.g., for Query R107, the output file name is “My_PRM_R107Ranking.dat”) for all 50 data collections and save them in the folder RankingOutputs”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formally describe your design for My_PRM** in an algorithm to **rank documents in each data collection *using corresponding query* (topic) ***for all 50 data collections*****.Your *approach should be generic*; that means it is feasible to be used for other topics (queries). You also need to **discuss the differences between My_PRM and the other two models (BM25 and JM_LM)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def My_PRM():\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Use Python to implement three models: `BM25`, `JM_LM`, and `My_PRM`, and **test them on the given 50 data collections for the corresponding 50 queries (topics)**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Python programs to implement these three models. You can use a .py file (or a .ipynb file) for each model.\n",
    "\n",
    "\n",
    "For each long query, your python programs will produce ranked results and save them into .dat files. For example, for query R107, you can save the ranked results of three models into “BM25_R107Ranking.dat”, “JM_LM_R107Ranking.dat”, and “My_PRM_R107Ranking.dat”, respectively by using the following format:\n",
    "- The first column is the document id (the itemid in the corresponding XML document)\n",
    "- The second column is the document score (or probability).\n",
    "\n",
    "**Describe:** \n",
    "- Python packages or modules (or any open-source software) you used\n",
    "- The data structures used to represent a single document and a set of documents for each model (you can use different data structures for different models).\n",
    "\n",
    "\n",
    "You also need to **test the three models on the given 50 data collections for the 50 queries (topics) by *printing out the top 15 documents* for each data collection (in descending order)**. The **output will also be put in the appendix of your final report**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 5: Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Use three effectiveness measures to evaluate the three models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, you need to **use the relevance judgments (EvaluationBenchmark.zip)** to **compare with the ranking outputs in the folder of “RankingOutputs” for the selected effectiveness metric** for the three models.\n",
    "\n",
    "\n",
    "You need to use the following three different effectiveness measures to evaluate the document ranking results you saved in the folder “RankingOutputs”:\n",
    "1) Average precision (and MAP)\n",
    "2) Precision@10 (and their average)\n",
    "3) Discounted cumulative gain at rank position 10 ($p = 10$), $DCG_{10}$ (and their average):  \n",
    "    $DCG_p=rel_i+\\sum_{i=2}^p\\frac{rel_i}{log_2(i)}$  \n",
    "        $rel_i=1$ if the document at position $i$ is releveant; otherwise, it is 0.\n",
    "\n",
    "Evaluation results can be summarized in tables or graphs. Examples are provided in the sepcification sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 6: Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Description:** Recommend a model based on significance test and your analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to conduct a significance test to compare models. You can choose a t-test to perform a significance test on the evaluation results (e.g., in Tables 1, 2 and 3). \n",
    "\n",
    "You can compare models between:\n",
    "- **BM25** and **JM_LM**\n",
    "- **BM25** and **My_PRM**\n",
    "- **JM_LM** and **My_PRM**\n",
    "\n",
    "Based on $t$-test results ($p$-value and $t$-statistic), you can recommend a model (You ***want the proposed \"My_RPM\" to be the best because it is your own model***). You can perform the $t$-test using a single effectiveness measure or multiple measures. Generally, using more effectiveness measures provides stronger evidence against the null hypothesis. Note that if the $t$-test is unsatisfactory, you can use the evaluation results to refine **My_PRM** mode. For example, you can adjust parameter settings or update your design and implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
